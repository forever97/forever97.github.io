---
title: BAN
date: 2020-08-06 11:56:02
tags: [VQA, NIPS, 2018, attention]
mathjax: true
cover: http://m.qpic.cn/psc?/V10nbMUG2wnlL5/bqQfVz5yrrGYSXMvKr.cqdLjtpOOdwNmMpnGO3XlOn9VrGajYLWk471i4KJUHe0ZM2*h9lLFPRIr8ARbm4D1Ur4FPmy1XFP07AjLwGnx88g!/b&bo=gAc4BAAAAAABB5s!&rf=viewer_4
---
[Paper Download Address](https://arxiv.org/abs/1805.07932)

VQA任务涉及到许多视觉-语言交叉的问题，因此attention在VQA中能够起到比较好的效果，co-attention可以同时推断视觉注意力和语言注意力，但同时忽略了语言和视觉区域之间的交互作用

作者将co-attention扩展为关注问题和图像的每一对多通道的bilinear attention(双线性注意力)，如果给定的问题涉及到由多个单词表示的多个视觉概念，则使用每个单词的视觉注意力分布进行推理比使用单个压缩的注意力分布进行推理更能挖掘出相关信息

作者在低秩双线性池化的基础上提出了双线性注意网络，BAN利用了两组输入通道之间的双线性交互，而低秩双线性池提取了每对通道的联合表示，此外作者还提出了一个多模态残差网络MRN来更有效地利用多重双线性注意图

BAN中用residual summations替代了concatenation，以更高效的参数和性能学习了eight-glimpse BAN，图中展示了一个two-glimpse BAN

![](http://m.qpic.cn/psc?/V10nbMUG3EIcUi/bqQfVz5yrrGYSXMvKr.cqc090e5*1Wz1srmyjxJqgmnVXX88qssar5ioiKn6WIR4FazXEV8AtJTKVM1Dwiej35.AqvFeEeiciDE4c66xt6Y!/b&bo=IQRnAQAAAAADB2E!&rf=viewer_4)

文章主要贡献如下：

1. 在低秩双线性池化技术的基础上，提出了学习和利用双线性注意分布的双线性注意网络

2. 提出了一种多模态残差网络(MRN)的变种，以有效地利用由模型产生的多双线性注意图，并成功地利用了多达8个注意力地图

3. 在VQA2.0上实现了SOTA，评估了双线性注意图在Flickr30k Entities上性能，推理速度提高了25.37%

## Low-rank bilinear pooling

低秩双线性池化算法使用单通道输入(question vector)组合其他多通道输入(image features)作为单通道中间表示(attended feature)

### Low-rank bilinear model

先前有研究提出了一个低秩双线性模型来降低双线性权矩阵$W_i$的秩，从而给出正则性，$W_i$被替换为两个更小矩阵的乘法$U_iV_i^T$，这里$U_i \in R^{N \times d}$，$V_i \in R^{M \times d}$, 这种替换使得$W_i$的秩$d \le min(M,N)$，标量输出$f_i$为

![](http://m.qpic.cn/psc?/V10nbMUG3EIcUi/TmEUgtj9EK6.7V8ajmQrEGAHsmwqi5yWL1lm330F3l7D5fIiTBm.oFXAs7cOe2WGJoX*j3X9Nz70bh2Fum6mORb7fMhV5K.g6.BR2StHJE4!/b&bo=ZQI8AAAAAAADF2k!&rf=viewer_4)

式子中的$1$是一个只包含1的向量，$\circ$ 表示Hadamard积 (element-wise multiplication)

### Low-rank bilinear pooling

对于向量输出f，引入了池化矩阵P

![](http://m.qpic.cn/psc?/V10nbMUG3EIcUi/TmEUgtj9EK6.7V8ajmQrELMQf4dMqX3vpAQwiA4jqG4WI8INaw3TClKOJkE*0y1im8P47VDTVXma9g67rLidVxR836QbYUWRcEOUNJQDIM8!/b&bo=TAE6AAAAAAADF0U!&rf=viewer_4)

$P \in R^{d \times c}$，$U \in R^{N \times d}$，$V \in R^{M \times d}$  

通过引入$P$作为向量输出$f \in R^c$允许U和V是二维张量，显著减少了参数的数量

### Unitary attention networks

注意力机制通过有选择地利用给定的信息来减少输入通道，假设有一个多通道输入Y，包含$|{y_i}|$个行向量，用注意力权重$\alpha$从Y中得到单通道$\hat{y}=\sum_i\alpha_iy_i$，注意力权重$\alpha$通过softmax计算得到

![](http://m.qpic.cn/psc?/V10nbMUG3EIcUi/TmEUgtj9EK6.7V8ajmQrECb3NGy6yxbRjnubhcSFT3PYbTJotZxKCeGc67Vc7Oj5CYvLCG5sPF5.FykqPqUjxfAVOUGYmw3cHmjoLbFiN2w!/b&bo=LgJAAAAAAAADF14!&rf=viewer_4)

$\alpha \in R^{G \times\phi}$，$P \in R^{d\times G}$，$U \in R^{N \times d}$，$x \in R^N$，$1 \in R^{\phi}$，$V \in R^{M \times d}$，$Y \in R^{M \times \phi}$，当$G>1$的时候表示采用了多glimpses(attention heads)，那么就有$\hat{y}=||_{g=1}^G\sum_i\alpha_{g,iYi}$，然后$x$和$\hat{y}$用低秩双线性池化来实现联合表示，最后进行分类

## Bilinear attention networks

作者推广了两个多通道输入的双线性模型，$X \in R^{N \times \rho}$以及$Y \in R^{M \times \phi}$，其中$\rho = |{x_i}|$以及$\phi =|{y_j}|$

为了同时减少两个输入通道，作者引入双线性注意映射

![](http://m.qpic.cn/psc?/V10nbMUG3EIcUi/TmEUgtj9EK6.7V8ajmQrELn7Z3Y2TcPzmuKEjZjttCaaHXs.3wkrXB52SFnVnr1JAjUNO4.U62eUno3JqsPgrx.4nzgWFMDV.T4572MCpu0!/b&bo=WAEuAAAAAAADF0U!&rf=viewer_4)

其中$U' \in R^{N \times K}$，$V' \in R^{M \times K}$，$(X^TU')_k \in R^{\rho}$，$(Y^TV')_k \in R^{\phi}$，$f_k'$表示第k个元素的中间表示，矩阵的下标k表示列的索引，$f_k'$也可以写作

![](http://m.qpic.cn/psc?/V10nbMUG3EIcUi/TmEUgtj9EK6.7V8ajmQrEC74oivEd54JhQAIsdnBxMrXRzmFI1KKiriTGSlzTjex.oRdeHbII4oVrQhbiUAaUQROA64LdY0yYNLpvwOBn6E!/b&bo=RQNoAAAAAAADFxw!&rf=viewer_4)

然后双线性联合表示$f=P^Tf'$，为方便起见，将双线性注意网络定义为由双线性注意映射参数化的两个多通道输入的函数
$BAN(X,Y;A)$

### Bilinear attention map

前文提到的注意图为

![](http://m.qpic.cn/psc?/V10nbMUG3EIcUi/TmEUgtj9EK6.7V8ajmQrEOV2bwYwqmp9uCMCNPTyYHqfjAXbKjVNZS8idLbYTuzccHyTVHqEyv7B25z0y24iu.20fTClkQB5Su.NCDoggmA!/b&bo=DwJHAAAAAAADF3g!&rf=viewer_4)

$softmax$前的每个$A_{ij}$都是通过低秩双线性池化得到的

![](http://m.qpic.cn/psc?/V10nbMUG3EIcUi/TmEUgtj9EK6.7V8ajmQrELTMKh4tcFjqcAKk9VI*BQMP62MZtePV1hi1U7DAiRyITd8YYHDd7lYfG7PG2*Xp.4AfnR*DVWq*66T*SMjsw68!/b&bo=sQE3AAAAAAADF7U!&rf=viewer_4)

多双线性注意图可以扩展为

![](http://m.qpic.cn/psc?/V10nbMUG3EIcUi/TmEUgtj9EK6.7V8ajmQrELzm2QmC2TcJ0wXws9CLRqKBYJLsSztluLR1SCvV6iwu4ra20nLT4XVO0lgqne*EPrQHlnN*iwFUa4gwSBuQeP0!/b&bo=DQI8AAAAAAADFwE!&rf=viewer_4)

其中$U$和$V$的参数是共享的，但$p_g$不共享，其中$g$表示glimpses的索引

### Residual learning of attention

作者使用MRN的变体从多重双线性注意图中得到联合表示，第i+1次的输出可以表示为

![](http://m.qpic.cn/psc?/V10nbMUG3EIcUi/TmEUgtj9EK6.7V8ajmQrEBH*QR4CanWrFe3KdS58iW6UY36WyozuZTBXKg8p*wCmvDOCw5r4QLHLIT0boLEDXlBqIDPkeyL753P0el3whLE!/b&bo=sgEtAAAAAAADF6w!&rf=viewer_4)

$f_0=X$，$1 \in R^\rho$

对最后一次输出的通道维度求和，就能得到分类器的输入

### Time complexity

$BAN$的复杂度等于一个多通道的输入大小$O(KM\phi)$

## Related works

### Multimodal factorized bilinear pooling

[Yu et al][1]做低秩双线性池化时移除了投影矩阵$P$，这个操作对$BAN$没有用

[1]:https://arxiv.org/abs/1708.03619

### Co-attention networks

[Xu and Saenko][2]提出了空间记忆网络模型，计算问题中每个token和每个图像patch之间的相关性，与本文不同的是，它是对相关矩阵的每一个行向量的最大值进行softmax

[2]:https://arxiv.org/abs/1511.05234

现有的co-attention都是对每个模态使用单独的注意分布，忽略了模态之间的相互作用

## Experiments

VQA2.0 Validation Score

![](http://m.qpic.cn/psc?/V10nbMUG3EIcUi/TmEUgtj9EK6.7V8ajmQrEALvQUzHu0.fcKkNKHcsFYYEgC0ER0QI1DbPYD1RyGOkjCCnz1i5FrG0QTY2CYtnFtR*pzlYbiWfe6hLNZmSO60!/b&bo=sgEYAQAAAAADF5g!&rf=viewer_4)

在VQA2.0上使用的参数量

![](http://m.qpic.cn/psc?/V10nbMUG3EIcUi/TmEUgtj9EK6.7V8ajmQrEKbnumRY9nrSKaDaqPsyD4p6Sx8cxQwoUKBh2MwF0mujUyYlnhOLgRIbGqho7DBB*FWujLHhXGQGIbruoPWl.nA!/b&bo=NAIPAQAAAAADFwo!&rf=viewer_4)

调参

![](http://m.qpic.cn/psc?/V10nbMUG3EIcUi/TmEUgtj9EK6.7V8ajmQrEC.6RbZ.g9cFbrmNSszpcZ1IQW064fJpFqix9ehPW7mkJpaNWeMoGPkrugb38GFcV2ldHwdiDl.SiZbLexX4LdE!/b&bo=rgSAAQAAAAADFxk!&rf=viewer_4)

two-glimpse BAN两次attention map可视化

![](http://m.qpic.cn/psc?/V10nbMUG3EIcUi/TmEUgtj9EK6.7V8ajmQrEHEBSGgRKz5ok.*QyFDShH7.5.FhF5DLuLihI89*ITXQvVkBURE0QSdy6vTXLAWZhvjIQOQmW.JzQTTkY6Lgm.Q!/b&bo=sgQNAQAAAAADJ7g!&rf=viewer_4)

Flickr30k Entities

![](http://m.qpic.cn/psc?/V10nbMUG3EIcUi/TmEUgtj9EK6.7V8ajmQrEFhnxBEFNJU1*7qvjK7YgOGsRotZaJB41mkRBlAq7FRwS3nmWRs7g1GbF.oFi5FwkLYmg0hG4aHkfJX6lPFn1a8!/b&bo=qwSAAQAAAAADR0w!&rf=viewer_4)

在VQA2.0 Test上和SOTA的比较

![](http://m.qpic.cn/psc?/V10nbMUG3EIcUi/TmEUgtj9EK6.7V8ajmQrEEQp1PlAeTwpX3UZBbm*rGTbdawxxfOSSx8mccRAtATMgaaM6q0eM7PdPUlXwbajC0XYPMg7of6fDMSuBzeUucc!/b&bo=7QNYAQAAAAADF4U!&rf=viewer_4)

